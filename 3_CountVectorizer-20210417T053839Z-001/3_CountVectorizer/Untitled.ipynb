{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(dataset):\n",
    "    unique_words = set()\n",
    "    \n",
    "    if isinstance(dataset, (list)):\n",
    "        for row in dataset:\n",
    "            for word in row.split(\" \"):\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                unique_words.add(word)\n",
    "                \n",
    "                #print(unique_words)\n",
    "                \n",
    "        unique_words = sorted(list(unique_words))\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "        \n",
    "        return vocab\n",
    "    else:\n",
    "        print(\"need a list\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = fit([\"abc def aaa prq\", \"lmn pqr aaaaaaa aaa abbb baaa\"])\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def transform(dataset,vocab):\n",
    "    rows = []\n",
    "    columns = []\n",
    "    values = []\n",
    "    if isinstance(dataset, (list)):\n",
    "        for idx, row in enumerate(tqdm(dataset)):\n",
    "            print(len(row.split(\" \")))\n",
    "            print(row)\n",
    "            word_freq = dict(Counter(row.split()))\n",
    "            #print(word_freq)\n",
    "            \n",
    "            for word, freq in word_freq.items():\n",
    "                if len(word) <2:\n",
    "                    continue\n",
    "                col_index = vocab.get(word, -1)\n",
    "                #print(col_index)\n",
    "                if col_index !=-1:\n",
    "                    # we are storing the index of the document\n",
    "                    rows.append(idx)\n",
    "                    # we are storing the dimensions of the word\n",
    "                    columns.append(col_index)\n",
    "                    # we are storing the frequency of the word\n",
    "                    values.append(freq)\n",
    "        return csr_matrix((values, (rows,columns)), shape=(len(dataset),len(vocab)))\n",
    "    else:\n",
    "        print(\"you need to pass list of strings\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = [\"the method of lagrange multipliers is the economists workhorse for solving optimization problems\",\n",
    "           \"the technique is a centerpiece of economic theory but unfortunately its usually taught poorly\"]\n",
    "vocab = fit(strings)\n",
    "print(list(vocab.keys()))\n",
    "print(transform(strings, vocab).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(dataset):\n",
    "    unique_words = set()\n",
    "    \n",
    "    if isinstance(dataset, (list)):\n",
    "        for row in dataset:\n",
    "            for word in row.split(\" \"):\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                unique_words.add(word)\n",
    "                \n",
    "                #print(unique_words)\n",
    "                \n",
    "        unique_words = sorted(list(unique_words))\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "        \n",
    "        return unique_words,vocab\n",
    "    else:\n",
    "        print(\"need a list\")\n",
    "    \n",
    "l= [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]\n",
    "u = fit(l)\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(corpus):\n",
    "    tfs = []\n",
    "    for document in corpus:\n",
    "        dic={}\n",
    "        for word in document.split():\n",
    "            if word in dic:\n",
    "                dic[word]+=1\n",
    "            else:\n",
    "                dic[word]=1\n",
    "        for word,freq in dic.items():\n",
    "            #print(word,freq)\n",
    "            dic[word]=freq/len(document.split())\n",
    "        tfs.append(dic)\n",
    "        \n",
    "    return tfs\n",
    "\n",
    "\n",
    "l= [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]\n",
    "#u = fit(l)\n",
    "a = tf(l)\n",
    "for i in range (0,4):\n",
    "    print(a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDF(corpus, unique_words):\n",
    "    idf_dict={}\n",
    "    #print(type(idf_dict))\n",
    "    N=len(corpus)\n",
    "    for i in unique_words:\n",
    "        count=0\n",
    "        for sen in corpus:\n",
    "            if i in sen.split():\n",
    "                count=count+1\n",
    "                idf_dict[i]=(math.log((1+N)/(count+1)))+1\n",
    "              \n",
    "    return idf_dict\n",
    "     \n",
    "\n",
    "l= [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'a': 3, 'b': 2, 'c': 3, 'd': 4, 'e': 5}\n",
    "first2pairs = {k: d[k] for k in list(d)[:2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first2pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator \n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SkLearn# Collection of string documents\n",
    "\n",
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(corpus):\n",
    "    unique_word = set()\n",
    "    \n",
    "    \n",
    "    if isinstance(corpus, (list)):\n",
    "        for row in corpus:\n",
    "            for word in row.split(\" \"):\n",
    "                if len(word)<2:\n",
    "                    continue\n",
    "                unique_word.add(word)\n",
    "        unique_word = sorted(list(unique_word))\n",
    "        #unique = list(unique_word)\n",
    "        idf = list()\n",
    "        \n",
    "        for w in unique_word:\n",
    "            x =1\n",
    "            n = 0\n",
    "            for doc in corpus:\n",
    "                if w in doc:\n",
    "                    n = n+1\n",
    "            x += (math.log((1+len(corpus))/(1+n)))\n",
    "            idf.append(x)\n",
    "            \n",
    "        vocab = {unique_word[i]:idf[i] for i in range(len(unique_word))}\n",
    "        return vocab\n",
    "        \n",
    "    else:\n",
    "        print(\"you need to pass list of sentance\")\n",
    "        \n",
    "        \n",
    "        \n",
    "u = fit(corpus)\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def transform(corpus):\n",
    "        vocab = fit(corpus)\n",
    "        rows = []\n",
    "        cols = []\n",
    "        idf_words = list(vocab.keys())\n",
    "        \n",
    "        non_zero = []\n",
    "        #sparse_matrix= csr_matrix( (len(corpus), len(vocab)), dtype=np.float64)\n",
    "    \n",
    "    \n",
    "        for wi, doc  in tqdm(enumerate(corpus)):\n",
    "            tfidf = []\n",
    "            word_freq = dict(Counter(doc.split()))\n",
    "            doc_length = len(doc.split(\" \"))\n",
    "            \n",
    "        \n",
    "            for word , freq in word_freq.items():\n",
    "                if len(word)<2:\n",
    "                    continue\n",
    "                idf_val = vocab.get(word, -1)\n",
    "                \n",
    "                if idf_val != -1:\n",
    "                    rows.append(wi)\n",
    "                    cols.append(idf_words.index(word))\n",
    "                    tf = freq/doc_length\n",
    "                    tfidf_val = tf * idf_val\n",
    "                    tfidf.append(tfidf_val)\n",
    "                    \n",
    "                    \n",
    "            l = len(tfidf)\n",
    "            if l == 0:\n",
    "                continue\n",
    "            tfidf = np.array(tfidf).reshape(-1, 1)        \n",
    "            tfidf = normalize(tfidf, norm=\"l2\", axis = 0).T[0] \n",
    "            non_zero.extend(tfidf)\n",
    "        return (csr_matrix((non_zero, (rows, cols)), shape = (len(corpus), len(vocab))))     \n",
    "                \n",
    "                    \n",
    "\n",
    "final_output=transform(corpus)\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sklearn Result:\\n\")\n",
    "print(skl_output[0])\n",
    "print(\"My Result:\\n\")\n",
    "print(transformed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator \n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus =  746\n"
     ]
    }
   ],
   "source": [
    "# Below is the code to load the cleaned_strings pickle file provided\n",
    "# Here corpus is of list type\n",
    "\n",
    "import pickle\n",
    "with open('cleaned_strings', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "    \n",
    "# printing the length of the corpus loaded\n",
    "print(\"Number of documents in corpus = \",len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['slow moving aimless movie distressed drifting young man', 'not sure lost flat characters audience nearly half walked', 'attempting artiness black white clever camera angles movie disappointed became even ridiculous acting poor plot lines almost non existent', 'little music anything speak', 'best scene movie gerardo trying find song keeps running head', 'rest movie lacks art charm meaning emptiness works guess empty', 'wasted two hours', 'saw movie today thought good effort good messages kids', 'bit predictable', 'loved casting jimmy buffet science teacher']\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zombiez : 6.922918004572872\n",
      "hugo : 6.922918004572872\n",
      "holds : 6.922918004572872\n",
      "hollander : 6.922918004572872\n",
      "homework : 6.922918004572872\n",
      "honestly : 6.922918004572872\n",
      "hopefully : 6.922918004572872\n",
      "hopeless : 6.922918004572872\n",
      "horrendously : 6.922918004572872\n",
      "horrid : 6.922918004572872\n",
      "horrified : 6.922918004572872\n",
      "hosting : 6.922918004572872\n",
      "houses : 6.922918004572872\n",
      "howdy : 6.922918004572872\n",
      "howell : 6.922918004572872\n",
      "humanity : 6.922918004572872\n",
      "hoffman : 6.922918004572872\n",
      "humans : 6.922918004572872\n",
      "hummh : 6.922918004572872\n",
      "hurt : 6.922918004572872\n",
      "hype : 6.922918004572872\n",
      "hypocrisy : 6.922918004572872\n",
      "idealogical : 6.922918004572872\n",
      "identified : 6.922918004572872\n",
      "identifies : 6.922918004572872\n",
      "idiotic : 6.922918004572872\n",
      "idyllic : 6.922918004572872\n",
      "imagine : 6.922918004572872\n",
      "imdb : 6.922918004572872\n",
      "impact : 6.922918004572872\n",
      "holding : 6.922918004572872\n",
      "hockey : 6.922918004572872\n",
      "plug : 6.922918004572872\n",
      "heels : 6.922918004572872\n",
      "handles : 6.922918004572872\n",
      "hankies : 6.922918004572872\n",
      "happiness : 6.922918004572872\n",
      "happy : 6.922918004572872\n",
      "harris : 6.922918004572872\n",
      "hatred : 6.922918004572872\n",
      "havilland : 6.922918004572872\n",
      "hayao : 6.922918004572872\n",
      "hayworth : 6.922918004572872\n",
      "heads : 6.922918004572872\n",
      "hearts : 6.922918004572872\n",
      "heartwarming : 6.922918004572872\n",
      "heche : 6.922918004572872\n",
      "heist : 6.922918004572872\n",
      "hilt : 6.922918004572872\n",
      "helen : 6.922918004572872\n",
      "{'zombiez': 6.922918004572872, 'hugo': 6.922918004572872, 'holds': 6.922918004572872, 'hollander': 6.922918004572872, 'homework': 6.922918004572872, 'honestly': 6.922918004572872, 'hopefully': 6.922918004572872, 'hopeless': 6.922918004572872, 'horrendously': 6.922918004572872, 'horrid': 6.922918004572872, 'horrified': 6.922918004572872, 'hosting': 6.922918004572872, 'houses': 6.922918004572872, 'howdy': 6.922918004572872, 'howell': 6.922918004572872, 'humanity': 6.922918004572872, 'hoffman': 6.922918004572872, 'humans': 6.922918004572872, 'hummh': 6.922918004572872, 'hurt': 6.922918004572872, 'hype': 6.922918004572872, 'hypocrisy': 6.922918004572872, 'idealogical': 6.922918004572872, 'identified': 6.922918004572872, 'identifies': 6.922918004572872, 'idiotic': 6.922918004572872, 'idyllic': 6.922918004572872, 'imagine': 6.922918004572872, 'imdb': 6.922918004572872, 'impact': 6.922918004572872, 'holding': 6.922918004572872, 'hockey': 6.922918004572872, 'plug': 6.922918004572872, 'heels': 6.922918004572872, 'handles': 6.922918004572872, 'hankies': 6.922918004572872, 'happiness': 6.922918004572872, 'happy': 6.922918004572872, 'harris': 6.922918004572872, 'hatred': 6.922918004572872, 'havilland': 6.922918004572872, 'hayao': 6.922918004572872, 'hayworth': 6.922918004572872, 'heads': 6.922918004572872, 'hearts': 6.922918004572872, 'heartwarming': 6.922918004572872, 'heche': 6.922918004572872, 'heist': 6.922918004572872, 'hilt': 6.922918004572872, 'helen': 6.922918004572872}\n"
     ]
    }
   ],
   "source": [
    "def fit50(corpus):\n",
    "    unique_word = set()\n",
    "    \n",
    "    \n",
    "    if isinstance(corpus, (list)):\n",
    "        for row in corpus:\n",
    "            for word in row.split(\" \"):\n",
    "                if len(word)<2:\n",
    "                    continue\n",
    "                unique_word.add(word)\n",
    "        unique_word = sorted(list(unique_word))\n",
    "        #unique = list(unique_word)\n",
    "        idf = list()\n",
    "        vocab = {}\n",
    "        for w in unique_word:\n",
    "            x =1\n",
    "            n = 0\n",
    "            for doc in corpus:\n",
    "                if w in doc:\n",
    "                    n = n+1\n",
    "            x += (math.log((1+len(corpus))/(1+n)))\n",
    "            idf.append(x)\n",
    "            \n",
    "        ind_ = np.argsort(idf)[::-1][:50] # index of top 50 idf scores\n",
    "        for i in ind_: # looping and adding them to the dict\n",
    "            print(unique_word[i], \":\", idf[i])\n",
    "            vocab[unique_word[i]] = idf[i]\n",
    "        return vocab\n",
    "    else:\n",
    "        print(\"you need to pass list of sentance\")\n",
    "        \n",
    "        \n",
    "        \n",
    "u = fit50(corpus)\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zombiez : 6.922918004572872\n",
      "hugo : 6.922918004572872\n",
      "holds : 6.922918004572872\n",
      "hollander : 6.922918004572872\n",
      "homework : 6.922918004572872\n",
      "honestly : 6.922918004572872\n",
      "hopefully : 6.922918004572872\n",
      "hopeless : 6.922918004572872\n",
      "horrendously : 6.922918004572872\n",
      "horrid : 6.922918004572872\n",
      "horrified : 6.922918004572872\n",
      "hosting : 6.922918004572872\n",
      "houses : 6.922918004572872\n",
      "howdy : 6.922918004572872\n",
      "howell : 6.922918004572872\n",
      "humanity : 6.922918004572872\n",
      "hoffman : 6.922918004572872\n",
      "humans : 6.922918004572872\n",
      "hummh : 6.922918004572872\n",
      "hurt : 6.922918004572872\n",
      "hype : 6.922918004572872\n",
      "hypocrisy : 6.922918004572872\n",
      "idealogical : 6.922918004572872\n",
      "identified : 6.922918004572872\n",
      "identifies : 6.922918004572872\n",
      "idiotic : 6.922918004572872\n",
      "idyllic : 6.922918004572872\n",
      "imagine : 6.922918004572872\n",
      "imdb : 6.922918004572872\n",
      "impact : 6.922918004572872\n",
      "holding : 6.922918004572872\n",
      "hockey : 6.922918004572872\n",
      "plug : 6.922918004572872\n",
      "heels : 6.922918004572872\n",
      "handles : 6.922918004572872\n",
      "hankies : 6.922918004572872\n",
      "happiness : 6.922918004572872\n",
      "happy : 6.922918004572872\n",
      "harris : 6.922918004572872\n",
      "hatred : 6.922918004572872\n",
      "havilland : 6.922918004572872\n",
      "hayao : 6.922918004572872\n",
      "hayworth : 6.922918004572872\n",
      "heads : 6.922918004572872\n",
      "hearts : 6.922918004572872\n",
      "heartwarming : 6.922918004572872\n",
      "heche : 6.922918004572872\n",
      "heist : 6.922918004572872\n",
      "hilt : 6.922918004572872\n",
      "helen : 6.922918004572872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "746it [00:00, 41544.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'zombiez': 6.922918004572872, 'hugo': 6.922918004572872, 'holds': 6.922918004572872, 'hollander': 6.922918004572872, 'homework': 6.922918004572872, 'honestly': 6.922918004572872, 'hopefully': 6.922918004572872, 'hopeless': 6.922918004572872, 'horrendously': 6.922918004572872, 'horrid': 6.922918004572872, 'horrified': 6.922918004572872, 'hosting': 6.922918004572872, 'houses': 6.922918004572872, 'howdy': 6.922918004572872, 'howell': 6.922918004572872, 'humanity': 6.922918004572872, 'hoffman': 6.922918004572872, 'humans': 6.922918004572872, 'hummh': 6.922918004572872, 'hurt': 6.922918004572872, 'hype': 6.922918004572872, 'hypocrisy': 6.922918004572872, 'idealogical': 6.922918004572872, 'identified': 6.922918004572872, 'identifies': 6.922918004572872, 'idiotic': 6.922918004572872, 'idyllic': 6.922918004572872, 'imagine': 6.922918004572872, 'imdb': 6.922918004572872, 'impact': 6.922918004572872, 'holding': 6.922918004572872, 'hockey': 6.922918004572872, 'plug': 6.922918004572872, 'heels': 6.922918004572872, 'handles': 6.922918004572872, 'hankies': 6.922918004572872, 'happiness': 6.922918004572872, 'happy': 6.922918004572872, 'harris': 6.922918004572872, 'hatred': 6.922918004572872, 'havilland': 6.922918004572872, 'hayao': 6.922918004572872, 'hayworth': 6.922918004572872, 'heads': 6.922918004572872, 'hearts': 6.922918004572872, 'heartwarming': 6.922918004572872, 'heche': 6.922918004572872, 'heist': 6.922918004572872, 'hilt': 6.922918004572872, 'helen': 6.922918004572872}, <746x50 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 50 stored elements in Compressed Sparse Row format>)\n"
     ]
    }
   ],
   "source": [
    " def transform(corpus):\n",
    "        vocab = fit50(corpus)\n",
    "        rows = []\n",
    "        cols = []\n",
    "        idf_words = list(vocab.keys())\n",
    "        \n",
    "        non_zero = []\n",
    "        #sparse_matrix= csr_matrix( (len(corpus), len(vocab)), dtype=np.float64)\n",
    "    \n",
    "    \n",
    "        for wi, doc  in tqdm(enumerate(corpus)):\n",
    "            tfidf = []\n",
    "            word_freq = dict(Counter(doc.split()))\n",
    "            doc_length = len(doc.split(\" \"))\n",
    "            \n",
    "        \n",
    "            for word , freq in word_freq.items():\n",
    "                if len(word)<2:\n",
    "                    continue\n",
    "                idf_val = vocab.get(word, -1)\n",
    "                \n",
    "                if idf_val != -1:\n",
    "                    rows.append(wi)\n",
    "                    cols.append(idf_words.index(word))\n",
    "                    tf = freq/doc_length\n",
    "                    tfidf_val = tf * idf_val\n",
    "                    tfidf.append(tfidf_val)\n",
    "                    \n",
    "                    \n",
    "            l = len(tfidf)\n",
    "            if l == 0:\n",
    "                continue\n",
    "            tfidf = np.array(tfidf).reshape(-1, 1)        \n",
    "            tfidf = normalize(tfidf, norm=\"l2\", axis = 0).T[0] \n",
    "            non_zero.extend(tfidf)\n",
    "        return vocab, (csr_matrix((non_zero, (rows, cols)), shape = (len(corpus), len(vocab))))     \n",
    "                \n",
    "                    \n",
    "\n",
    "final_output=transform(corpus)\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
