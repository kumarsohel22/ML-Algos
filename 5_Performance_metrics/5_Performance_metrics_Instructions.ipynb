{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0Ej_bXyQvnV"
   },
   "source": [
    "# Compute performance metrics for the given Y and Y_score without sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CHb6NE7Qvnc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# other than these two you should not import any other packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KbsWXuDaQvnq"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>A.</b></font> Compute performance metrics for the given data <strong>5_a.csv</strong>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points >> number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_a.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a> Note: it should be numpy.trapz(tpr_array, fpr_array) not numpy.trapz(fpr_array, tpr_array)</li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaFLW7oBQvnt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10100, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "data = pd.read_csv(\"5_a.csv\")  # Read csv file\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['y', 'proba'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)  # checking the columns name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    10000\n",
       "0.0      100\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"y\"].value_counts()  # counting the number of ones and zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will the convert probability score(proba in data) into 1 & 0 according to the thresh-hold\n",
    "\n",
    "def predict(df,y,threshold):  \n",
    "    y_pred = []\n",
    "    for label in df[y]:   # iterating through the proba\n",
    "        if label<threshold:   # checking the threshhold is greater than the probability score \n",
    "            y_pred.append(0)   # if yes then append 0 in y_pred\n",
    "        else:\n",
    "            y_pred.append(1)    # else append 1 in y_pred\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: y_pred, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['y_pred'] = predict(data,'proba',0.5)\n",
    "data['y_pred'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will return the confusion matrix \n",
    "\n",
    "def con_matrix(df,y,y_pred):\n",
    "    tp=0\n",
    "    tn=0\n",
    "    fn=0\n",
    "    fp=0\n",
    "    for val1,val2 in enumerate(df[y]):\n",
    "        if(df.y_pred[val1]==1) and df.y[val1]==1:\n",
    "            tp=tp+1\n",
    "        if(df.y_pred[val1]==0) and df.y[val1]==0:\n",
    "            tn=tn+1\n",
    "        if(df.y_pred[val1]==0) and df.y[val1]==1:\n",
    "            fn=fn+1\n",
    "        if(df.y_pred[val1]==1) and df.y[val1]==0:\n",
    "            fp=fp+1\n",
    "    return {'tn':tn,'tp':tp,'fn':fn,'fp':fp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tn': 0, 'tp': 10000, 'fn': 0, 'fp': 100}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_matrix(data,'y','y_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will return the F1 score\n",
    "\n",
    "def f1_score(df,confusion_matrix):\n",
    "    x=df.y.value_counts()\n",
    "    P=x[1]\n",
    "\n",
    "    precision=confusion_matrix['tp']/(confusion_matrix['tp']+confusion_matrix['fp'])\n",
    "    recall=confusion_matrix['tp']/P\n",
    "\n",
    "    F1=2*precision*recall/(precision+recall)\n",
    "    print('the F1 score is: ',F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the F1 score is:  0.9950248756218906\n"
     ]
    }
   ],
   "source": [
    "Z = con_matrix(data,'y','y_pred')\n",
    "f1_score(data,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy is:  0.9900990099009901\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "Acc=(Z['tp']+Z['tn'])/data.shape[0]\n",
    "print('the accuracy is: ',Acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def auc(df):\n",
    "    s = df['y'].value_counts()\n",
    "    P = s[1]\n",
    "    N = s[0]\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for elem in tqdm(df['proba']):\n",
    "        df['y_pred']=predict(df,'proba',elem)\n",
    "        confusion_matrix=con_matrix(df,'y','y_pred')\n",
    "        tpr.append(confusion_matrix['tp']/P)\n",
    "        fpr.append(confusion_matrix['fp']/N)\n",
    "        df.drop(columns=['y_pred'])\n",
    "    return np.trapz(tpr,fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10100/10100 [2:17:38<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the AUC Score is : 0.48829900000000004\n"
     ]
    }
   ],
   "source": [
    "data=data.sort_values(by='proba',ascending=False)\n",
    "data.drop(columns=['y_pred'])\n",
    "AUC_score=auc(data)\n",
    "print ('the AUC Score is :',AUC_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V5KZem1BQvn2"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>B.</b></font> Compute performance metrics for the given data <strong>5_b.csv</strong>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points << number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_b.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a></li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2sKlq0YQvn5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10100, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code\n",
    "data_B = pd.read_csv('5_b.csv')\n",
    "data_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['y', 'proba'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data_B.columns)  # checking the columns name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    10000\n",
       "1.0      100\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_B[\"y\"].value_counts()  # counting the number of ones and zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: y_pred, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_B['y_pred'] = predict(data_B,'proba',0.5)\n",
    "data_B['y_pred'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tn': 9761, 'tp': 55, 'fn': 45, 'fp': 239}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_matrix(data_B,'y','y_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the F1 score is:  0.2791878172588833\n"
     ]
    }
   ],
   "source": [
    "Z_B = con_matrix(data_B,'y','y_pred')\n",
    "f1_score(data_B,Z_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Accuracy is : 0.9718811881188119\n"
     ]
    }
   ],
   "source": [
    "Acc_B=(Z_B['tp']+Z_B['tn'])/data_B.shape[0]\n",
    "print('the Accuracy is :',Acc_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10100/10100 [2:11:05<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the AUC Score is:  0.9377570000000001\n"
     ]
    }
   ],
   "source": [
    "data_B=data_B.sort_values(by='proba',ascending=False)\n",
    "data_B.drop(columns=['y_pred'])\n",
    "AUC_score_B=auc(data_B)\n",
    "print('the AUC Score is: ',AUC_score_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GiPGonTzQvoB"
   },
   "source": [
    "<font color='red'><b>C.</b></font> Compute the best threshold (similarly to ROC curve computation) of probability which gives lowest values of metric <b>A</b> for the given data <strong>5_c.csv</strong>\n",
    "<br>\n",
    "\n",
    "you will be predicting label of a data points like this: $y^{pred}= \\text{[0 if y_score < threshold  else 1]}$\n",
    "\n",
    "$ A = 500 \\times \\text{number of false negative} + 100 \\times \\text{numebr of false positive}$\n",
    "\n",
    "<pre>\n",
    "   <b>Note 1:</b> in this data you can see number of negative points > number of positive points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_c.csv</b>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5HIJzq1QvoE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y      prob\n",
      "0  0  0.458521\n",
      "1  0  0.505037\n",
      "2  0  0.418652\n",
      "3  0  0.412057\n",
      "4  0  0.375579\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('5_c.csv')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_metric(data):\n",
    "    metric={}    # declaring the empty dictionary for storing value\n",
    "    for elem in tqdm(data['prob']):\n",
    "        data['y_pred']= predict(data,'prob',elem)  # convert probability score(proba in data) into 1 & 0 where thresh-hold is elem\n",
    "        con_mat = con_matrix(data,'y','y_pred')   # computing the confusion marix\n",
    "        metric_val=(500*con_mat['fn'])+(100*con_mat['fp'])  # implimenting the given formula\n",
    "        metric[elem]=metric_val  \n",
    "        data.drop(columns=['y_pred'])\n",
    "    return(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2852/2852 [14:40<00:00,  4.31it/s]\n"
     ]
    }
   ],
   "source": [
    "result=low_metric(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the key:value pair for min value of the specified metric is- [0.2300390278970873] 141000\n"
     ]
    }
   ],
   "source": [
    "temp = min(result.values()) \n",
    "res = [key for key in result if result[key] == temp]\n",
    "print('the key:value pair for min value of the specified metric is-',res,temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sD4CcgjXQvoL"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>D.</b></font> Compute performance metrics(for regression) for the given data <strong>5_d.csv</strong>\n",
    "    <b>Note 2:</b> use pandas or numpy to read the data from <b>5_d.csv</b>\n",
    "    <b>Note 1:</b> <b>5_d.csv</b> will having two columns Y and predicted_Y both are real valued features\n",
    "<ol>\n",
    "<li> Compute Mean Square Error </li>\n",
    "<li> Compute MAPE: https://www.youtube.com/watch?v=ly6ztgIkUxk</li>\n",
    "<li> Compute R^2 error: https://en.wikipedia.org/wiki/Coefficient_of_determination#Definitions </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131.0</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>164.0</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154.0</td>\n",
       "      <td>152.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       y   pred\n",
       "0  101.0  100.0\n",
       "1  120.0  100.0\n",
       "2  131.0  113.0\n",
       "3  164.0  125.0\n",
       "4  154.0  152.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_d = pd.read_csv(\"5_d.csv\")\n",
    "data_d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ss_res(df,col):\n",
    "    val=0\n",
    "    for index,value in enumerate(df[col]):\n",
    "        val=val+(value*value)\n",
    "    return val\n",
    "\n",
    "def ss_tot(df,col):\n",
    "    val=0\n",
    "    mean_val=data_d['y'].mean()\n",
    "    for index,value in enumerate(df[col]):\n",
    "        val=val+ (value-mean_val)*(value-mean_val)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(df,col1,col2):\n",
    "    val=[]\n",
    "    for index, (value1, value2) in enumerate(zip(df[col1], df[col2])):\n",
    "        val.append(value1-value2)\n",
    "    return val\n",
    "    \n",
    "def absolute_error(df,col):\n",
    "    val=[]\n",
    "    for index,value in enumerate(df[col]):\n",
    "        val.append(abs(value))\n",
    "    return val\n",
    "\n",
    "\n",
    "data_d['error']=error(data_d,'y','pred')\n",
    "data_d['abs_error']=absolute_error(data_d,'error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1291202994009687\n"
     ]
    }
   ],
   "source": [
    "# computing MAPE value\n",
    "val=sum(data_d['abs_error'])/sum(data_d['y'])\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Co-efficient of determination value is:  0.9563582786990964\n"
     ]
    }
   ],
   "source": [
    "# computing r^2 value\n",
    "SS_RES=ss_res(data_d,'error')\n",
    "SS_TOT=ss_tot(data_d,'y')\n",
    "R_square= 1- (SS_RES/SS_TOT)\n",
    "print('the Co-efficient of determination value is: ',R_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177.16569974554707\n"
     ]
    }
   ],
   "source": [
    "# computing mean square error\n",
    "MSE = ss_res(data_d,'error')/len(data_d['error'])\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5_Performance_metrics_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
